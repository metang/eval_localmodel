# Model configuration for eval-localmodel.
#
# This file defines which models to download and evaluate.
# Each runtime section lists models available for that backend.
# The run_eval script generates a compare YAML from this automatically.

models:
  # ── Ollama ──────────────────────────────────────────────────────
  # Models are pulled via `ollama pull <name>`.
  # Ensure `ollama serve` is running before evaluation.
  ollama:
    - name: "qwen2.5:0.5b"
    - name: "qwen2.5:1.5b"

  # ── Foundry Local ───────────────────────────────────────────────
  # Models are auto-downloaded by the SDK on first load_model().
  # Specify which device variants to test (cpu, gpu, npu).
  foundry-local:
    - name: qwen2.5-0.5b
      devices: [cpu, gpu, npu]
    - name: qwen2.5-1.5b
      devices: [cpu, gpu]

  # ── llama-cpp-python ────────────────────────────────────────────
  # Provide a HuggingFace GGUF URL and local path.
  # The download script will fetch the file if it doesn't exist.
  # llama-cpp:
  #   - name: qwen2.5-0.5b-q4km
  #     gguf_url: "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf"
  #     gguf_path: "models/qwen2.5-0.5b-instruct-q4_k_m.gguf"
  #     base_url: "http://localhost:8000/v1"

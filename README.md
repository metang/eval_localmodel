# eval_localmodel

Modular evaluation framework for testing local LLM **tool-calling accuracy and agentic orchestration** across different runtimes.

## Supported Runtimes

| Runtime | Mode | How It Connects |
|---|---|---|
| **Ollama** | OpenAI-compat API | `http://localhost:11434/v1` |
| **llama-cpp-python** | In-process or server | Direct `Llama()` or `http://localhost:8000/v1` |
| **Foundry Local (ORT)** | OpenAI-compat API | Auto-start via `foundry-local-sdk` or manual endpoint |

## Quick Start (Scripts)

Three scripts automate the full workflow (available in both PowerShell and Bash):

**Bash (macOS / Linux):**
```bash
# 1. One-time setup — creates conda env, installs deps, generates config
./scripts/setup.sh

# 2. Edit config/models.yaml to choose which models & devices to evaluate

# 3. Download / pull all models listed in the config
./scripts/download_models.sh

# 4. Run evaluation and open HTML report in browser
./scripts/run_eval.sh                   # auto-timestamped output
./scripts/run_eval.sh -t experiment1    # custom tag
```

**PowerShell (Windows):**
```powershell
# 1. One-time setup — creates conda env, installs deps, generates config
.\scripts\setup.ps1

# 2. Edit config/models.yaml to choose which models & devices to evaluate

# 3. Download / pull all models listed in the config
.\scripts\download_models.ps1

# 4. Run evaluation and open HTML report in browser
.\scripts\run_eval.ps1                      # auto-timestamped output
.\scripts\run_eval.ps1 -Tag "experiment1"   # custom tag
```

Outputs are saved to `results/`:
- `results/report_<tag>.html` — interactive HTML report (opens automatically)
- `results/results_<tag>.csv` — raw results for further analysis

## Model Configuration

Models are defined in `config/models.yaml`. The file is auto-generated by `setup.ps1` if it doesn't exist. Example:

```yaml
models:
  ollama:
    - name: "qwen2.5:0.5b"
    - name: "qwen2.5:1.5b"

  foundry-local:
    - name: qwen2.5-0.5b
      devices: [cpu, gpu, npu]

  # llama-cpp:
  #   - name: qwen2.5-0.5b-q4km
  #     gguf_url: "https://huggingface.co/..."
  #     gguf_path: "models/qwen2.5-0.5b-q4km.gguf"
  #     base_url: "http://localhost:8000/v1"
```

The `run_eval` script auto-generates a compare YAML from this file. Foundry Local entries expand one run per device variant; unavailable devices are skipped gracefully.

## Manual CLI Usage

```bash
conda activate eval_localmodel

# Single model evaluation
python -m src.cli run -r ollama -m llama3.1

# Specific test suites
python -m src.cli run -r ollama -m llama3.1 -s simple_single -s tool_selection

# Foundry Local with device selection
python -m src.cli run -r foundry-local -m qwen2.5-0.5b --device gpu

# Compare models side-by-side (YAML config)
python -m src.cli compare -c config/compare_qwen25_05b.yaml

# Export results
python -m src.cli compare -c config/compare_qwen25_05b.yaml --csv results.csv --html report.html
```

## Test Suites

| Suite | Tests | What It Measures |
|---|---|---|
| `simple_single` | 3 | Single tool call with correct name + arguments |
| `tool_selection` | 2 | Picking the right tool from multiple options |
| `multi_tool` | 2 | Parallel / multi-function calls in one turn |
| `negative` | 3 | Correctly refusing when no tool applies |

Add new suites by dropping a `.json` file into `src/test_suites/data/`. See `doc/test_cases.md` for detailed test case documentation.

## Architecture

```
src/
├── models.py              # Shared data models (TestCase, EvalResult, etc.)
├── cli.py                 # Click CLI entry point
├── runtimes/              # Plugin-based runtime backends
│   ├── base.py            # Abstract BaseRuntime interface
│   ├── registry.py        # Runtime discovery & factory
│   ├── parsing.py         # Shared tool-call parsing utilities
│   ├── ollama_rt.py       # Ollama backend
│   ├── llamacpp_rt.py     # llama-cpp-python backend
│   └── foundry_rt.py      # Foundry Local backend (SDK auto-start + device selection)
├── eval/                  # Evaluation engine
│   ├── runner.py          # Test runner with per-test progress output
│   ├── matchers.py        # Argument matching (exact/fuzzy/type-only)
│   └── results.py         # Aggregation & summary statistics
├── test_suites/           # Test case loader + data
│   ├── __init__.py        # Suite loader API
│   └── data/*.json        # Test case definitions
├── reporting/             # Output formatting
│   ├── console.py         # Rich console tables + CSV export
│   └── html.py            # Self-contained HTML report generator
config/
├── models.yaml            # Which models to download & evaluate
├── compare_*.yaml         # Compare run configs (manual or auto-generated)
scripts/
├── setup.sh / setup.ps1              # Environment setup
├── download_models.sh / .ps1         # Model download/pull
└── run_eval.sh / .ps1                 # Run evaluation + generate report
doc/
└── test_cases.md          # Test case documentation
```

## Adding a New Runtime

1. Create `src/runtimes/my_rt.py`
2. Subclass `BaseRuntime` and implement `chat_with_tools()`, `list_models()`, `health_check()`
3. Decorate with `@register_runtime("my-runtime")`
4. Import in `src/runtimes/registry.py`

## Adding a New Test Suite

Create a JSON file in `src/test_suites/data/` following this schema:

```json
[
  {
    "id": "unique_test_id",
    "category": "my_category",
    "description": "What this tests",
    "messages": [{"role": "user", "content": "..."}],
    "tools": [{"type": "function", "function": {...}}],
    "expected_tool_calls": [{"name": "fn", "arguments": {"key": "val"}}],
    "match_level": "fuzzy",
    "is_negative": false
  }
]
```
